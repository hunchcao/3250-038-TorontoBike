{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "##load datasets to dataframes\n",
    "ride1=pd.read_csv(r'E:\\DS 3250\\Project\\bikeshare-ridership-2017\\2017 Data\\Bikeshare Ridership (2017 Q1).csv',sep=',')\n",
    "ride2=pd.read_csv(r'E:\\DS 3250\\Project\\bikeshare-ridership-2017\\2017 Data\\Bikeshare Ridership (2017 Q2).csv',sep=',')\n",
    "ride3=pd.read_csv(r'E:\\DS 3250\\Project\\bikeshare-ridership-2017\\2017 Data\\Bikeshare Ridership (2017 Q3).csv',sep=',')\n",
    "ride4=pd.read_csv(r'E:\\DS 3250\\Project\\bikeshare-ridership-2017\\2017 Data\\Bikeshare Ridership (2017 Q4).csv',sep=',')\n",
    "ride=pd.concat([ride1,ride2,ride3,ride4])\n",
    "\n",
    "w1=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_01-2017_P1H.csv',sep=',')\n",
    "w2=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_02-2017_P1H.csv',sep=',')\n",
    "w3=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_03-2017_P1H.csv',sep=',')\n",
    "w4=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_04-2017_P1H.csv',sep=',')\n",
    "w5=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_05-2017_P1H.csv',sep=',')\n",
    "w6=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_06-2017_P1H.csv',sep=',')\n",
    "w7=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_07-2017_P1H.csv',sep=',')\n",
    "w8=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_08-2017_P1H.csv',sep=',')\n",
    "w9=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_09-2017_P1H.csv',sep=',')\n",
    "w10=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_10-2017_P1H.csv',sep=',')\n",
    "w11=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_11-2017_P1H.csv',sep=',')\n",
    "w12=pd.read_csv(r'E:\\DS 3250\\Project\\Toronto weather 2017 hourly\\en_climate_hourly_ON_6158355_12-2017_P1H.csv',sep=',')\n",
    "weather=pd.concat([w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12])\n",
    "\n",
    "station_json=pd.read_json(r'E:\\DS 3250\\Project\\station_information.json')\n",
    "station = pd.concat([pd.DataFrame(json_normalize(x)) for x in station_json['data']],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## weather dataset at glance\n",
    "weather.info()\n",
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Temperature attribute analysis-box plot\n",
    "\n",
    "fig=px.box(weather,y='Temp (째C)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Temperature attribute analysis-histogram plot\n",
    "\n",
    "fig=px.histogram(weather,x='Temp (째C)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NA info\n",
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Change weather from hourly to daily,only take temperature attribute\n",
    "weather['Date/Time']=pd.to_datetime(weather['Date/Time'])\n",
    "weather['Date']=weather['Date/Time'].dt.date\n",
    "weather_daily=weather.groupby(['Date'],as_index=False)['Temp (째C)'].mean()\n",
    "weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##station dataset\n",
    "station.info()\n",
    "station.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##change data type\n",
    "station['station_id']=station['station_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NA information\n",
    "station.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##ridership dataset at glance, the trip duration is pretty dirty.\n",
    "ride.info()\n",
    "ride.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## begore drop noisy data\n",
    "ride.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop noisy data from trip_duration_seconds using + - 1.5 * interquartile. \n",
    "## Also, based on real life, any renting < 60s should be invalid data regards to trip time.\n",
    "q1=ride['trip_duration_seconds'].quantile(0.25)\n",
    "q3=ride['trip_duration_seconds'].quantile(0.75)\n",
    "interquatile_range=q3-q1\n",
    "ride=ride[~((ride['trip_duration_seconds']<(q1-1.5*interquatile_range))|(ride['trip_duration_seconds']>(q3+1.5*interquatile_range))\n",
    "            |(ride['trip_duration_seconds']<60))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After drop, roughly 7% data got dropped.\n",
    "ride.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trip duration attribute analysis-box plot\n",
    "fig=px.box(ride,y='trip_duration_seconds')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Trip duration attribute analysis-histogram plot\n",
    "\n",
    "fig=px.histogram(ride,x='trip_duration_seconds')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NA info, a lot of station_id are missing. Since the the station name in ridership table is not standard name cannot directly join with\n",
    "## station table, so have to do some fussy logics.\n",
    "ride.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##change data type\n",
    "station['station_id']=station['station_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Generate unique station information from ridership table\n",
    "all_stations=ride[['from_station_id','from_station_name','trip_id']].append(ride[['to_station_id','to_station_name','trip_id']].rename(\n",
    "    columns={'to_station_id':'from_station_id','to_station_name':'from_station_name'}))\n",
    "unique_stations=all_stations.fillna(1).groupby(['from_station_id','from_station_name'],as_index=False)['trip_id'].count()\n",
    "unique_stations.drop(unique_stations[unique_stations['from_station_name']==1].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Use fuzzywuzzy library to do fuzzy match for station name, the threshold set as 90 score, we try 80, have some mismatch.\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "no_ids = unique_stations[unique_stations['from_station_id']==1]\n",
    "for idx, miss in no_ids.iterrows():\n",
    "    max_score = 0\n",
    "    \n",
    "    for i, exist in station[['station_id', 'name']].iterrows():\n",
    "        score = fuzz.ratio(miss['from_station_name'], exist['name'])\n",
    "        \n",
    "        if score > 90 and score > max_score:\n",
    "            max_score = score\n",
    "            no_ids.at[idx, 'from_station_id'] = exist['station_id']\n",
    "    \n",
    "    if max_score <= 90:\n",
    "        print('Warnning: {0} station could not be matched to an existing station'.format(miss['from_station_name']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Check the fuzzy effectiveness, show the matched results\n",
    "unique_stations_cleaned=pd.concat([no_ids[no_ids['from_station_id']!=1],\n",
    "        unique_stations[unique_stations['from_station_id']!=1]])\n",
    "unique_stations_cleaned=unique_stations_cleaned.drop(columns=['trip_id'])\n",
    "unique_stations_cleaned=unique_stations_cleaned.drop_duplicates().reset_index(drop=True)\n",
    "n=unique_stations_cleaned.groupby('from_station_id')['from_station_name'].nunique()\n",
    "unique_stations_cleaned[unique_stations_cleaned['from_station_id'].isin(n.index[n.gt(1)])].sort_values('from_station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stations_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##unique_stations_cleaned is the dataframe of station from ridership table,combine the lat, lon information from station table\n",
    "unique_stations_combined=pd.merge(unique_stations_cleaned,station[['station_id','name','lat','lon']],\n",
    "                                 how='inner',left_on='from_station_id',right_on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_stations_combined.rename(columns={'from_station_id':'original_station_id','from_station_name':'original_name'},inplace=True)\n",
    "unique_stations_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Drop invalid data from trip_stop_time\n",
    "ride.drop(ride[ride['trip_stop_time']== 'NULLNULL'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formating the data type\n",
    "ride['from_station_id']=ride['from_station_id'].astype('int64',errors='ignore')\n",
    "ride['to_station_id']=ride['to_station_id'].astype('int64',errors='ignore')\n",
    "ride['trip_stop_time']=pd.to_datetime(ride['trip_stop_time'],dayfirst=True)\n",
    "ride['trip_start_time']=pd.to_datetime(ride['trip_start_time'],dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Join with station for start from\n",
    "data_join=pd.merge(ride,unique_stations_combined[['station_id','original_name','name','lat','lon']],how='left',\n",
    "                   left_on='from_station_name',right_on='original_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Join with station for stop at\n",
    "data_join=pd.merge(data_join,unique_stations_combined[['station_id','original_name','name','lat','lon']],how='left',\n",
    "                   left_on='to_station_name',right_on='original_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add the date of trip start\n",
    "data_join['trip_start_date']=data_join['trip_start_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Join with weather daily\n",
    "data_join=pd.merge(data_join,weather_daily[['Date','Temp (째C)']],\n",
    "                                 how='inner',left_on='trip_start_date',right_on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##delete unnecessary column, rename columns\n",
    "data_join.rename(columns={'station_id_x':'from_station_id_corr','station_id_y':'to_station_id_corr',\n",
    "                          'lat_x':'from_lat','lon_x':'from_lon',\n",
    "                          'lat_y':'to_lat','lon_y':'to_lon',\n",
    "                         'name_x':'from_station_name_corr','name_y':'to_station_name_corr'},inplace=True)\n",
    "data_join.drop(columns=['original_name_x','original_name_y','Date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_join.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_join.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create work day and month for visualization purpose\n",
    "data_join['work_day'] = data_join['trip_start_time'].apply(lambda x: x.strftime('%a'))\n",
    "data_join['month']=data_join['trip_start_time'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save to csv\n",
    "data_join.to_csv(r'E:\\DS 3250\\Project\\data_join.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##file is two large, split into two\n",
    "data_join[:700000].to_csv(r'E:\\DS 3250\\Project\\data_join1.csv',index=False)\n",
    "data_join[700000:].to_csv(r'E:\\DS 3250\\Project\\data_join2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
